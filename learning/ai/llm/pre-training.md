# Pre-training

### Pre-training(사전학습):

* 사전 학습은 대규모의 text dataset을 사용해 모델이 일반적인 언어 이해 능력을 학습하는 과정
  * 이 단계에서는 특정 작업(ex: 번역, 감정 분석 등)을 염두에 두지 않고,\
    **단순히 언어의 패턴과 구조를 학습하는 것이 목적**
    * :fire:사전 학습을 통해 모델은 다양한 텍스트에서 **언어의 기본적인 규칙**을 배우고, \
      이후에 **특정 작업에 빠르게 적응할 수 있는 기반**을 다진다
      * Hugging Face에서 제공하는 대부분의 모델들은 이 단계까지 완료된 상태로 제공됨

**특징:**

1. **대규모 데이터셋 사용**

* 인터넷에서 수집한 방대한 양의 텍스트 데이터로 모델을 학습

2. **일반적인 언어 이해**

* 모델은 텍스트 내 단어의 의미, 문장 구조, 문맥 등 언어의 전반적인 특징을 학습

3. **작업 비특화**

* 특정 작업에 맞춰진 학습이 아닌, 전반적인 언어 이해에 초점



***









