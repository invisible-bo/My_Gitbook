---
icon: star-exclamation
---

# Ollama

### Ollama

[Ollama](https://ollama.com/)

* **Ollama**는 로컬에서 실행할 수 있는 **LLM(대형 언어 모델) 실행 프레임워크**

🔥 **주요 특징:**

* **로컬 실행:** 클라우드 없이 로컬 머신에서 LLM 실행 가능
* **간편한 모델 관리:** `ollama pull model_name`으로 다운로드 & 실행
* **경량화:** 최적화된 모델을 사용해 성능 대비 낮은 자원 소비
* **GPU 지원:** CUDA 사용 가능(엔비디아 GPU 환경)
* **개발 친화적:** API 제공 & CLI 기반 실행 지원



실행

```sh
ollama run llama2
```









