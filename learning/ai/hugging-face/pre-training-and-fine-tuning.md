# Pre-training & Fine-tuning

### Pre-training(사전학습):

* 사전 학습은 대규모의 text dataset을 사용해 모델이 일반적인 언어 이해 능력을 학습하는 과정
  * 이 단계에서는 특정 작업(ex: 번역, 감정 분석 등)을 염두에 두지 않고,\
    **단순히 언어의 패턴과 구조를 학습하는 것이 목적**
    * :fire:사전 학습을 통해 모델은 다양한 텍스트에서 **언어의 기본적인 규칙**을 배우고, \
      이후에 **특정 작업에 빠르게 적응할 수 있는 기반**을 다진다
      * Hugging Face에서 제공하는 대부분의 모델들은 이 단계까지 완료된 상태로 제공됨

**특징:**

1. **대규모 데이터셋 사용**

* 인터넷에서 수집한 방대한 양의 텍스트 데이터로 모델을 학습

2. **일반적인 언어 이해**

* 모델은 텍스트 내 단어의 의미, 문장 구조, 문맥 등 언어의 전반적인 특징을 학습

3. **작업 비특화**

* 특정 작업에 맞춰진 학습이 아닌, 전반적인 언어 이해에 초점



***

### Fine-tuning:

* 사전 학습된 모델을 **특정 작업**에 맞게 **추가로 학습시키는 과정**
* 예를 들어, BERT 모델을 감정 분석에 사용하려면, BERT의 사전 학습된 가중치를 유지하면서 감정 분석 작업에 맞게 모델의 가중치를 조정
  * :fire:특정 작업에서 최상의 성능을 발휘하도록 모델을 조정하는 과정
    * 사전 학습 덕분에, 파인 튜닝은 더 빠르고 적은 데이터로 이루어질 수 있다
      * NLP 모델은 파인 튜닝 과정을 거쳐 실제 애플리케이션에서 사용

특징:

1. 작업 특화

* 파인 튜닝은 특정 작업(예: 텍스트 분류, 번역, 질의 응답 등)에 맞춰 **모델을 최적화하는 과정**

2. 사전 학습(Pre-training) 가중치 활용

* 사전 학습된 모델의 언어 이해 능력을 바탕으로, 새로운 작업에 적응할 수 있도록 **일부 가중치만 조정**

3. 적은 데이터로도 가능

* 사전 학습 덕분에, 파인 튜닝은 비교적 적은 양의 데이터로도 효과적인 학습이 가능







